package com.feri.watchmyparent.mobile.infrastructure.services;

import android.util.Log;
import com.feri.watchmyparent.mobile.application.dto.SensorDataDTO;
import com.feri.watchmyparent.mobile.infrastructure.kafka.RealHealthDataKafkaProducer;
import com.feri.watchmyparent.mobile.infrastructure.services.OfflineDataManager.OfflineHealthData;
import java.time.LocalDateTime;
import java.util.ArrayList;
import java.util.List;
import java.util.concurrent.CompletableFuture;
import java.util.concurrent.Executors;
import java.util.concurrent.ScheduledExecutorService;
import java.util.concurrent.TimeUnit;
import javax.inject.Inject;
import javax.inject.Singleton;

//GestioneazƒÉ retry logic-ul pentru transmiterea datelor cƒÉtre Kafka
//Include exponential backoff »ôi dead letter queue
@Singleton
public class KafkaRetryService {

    private static final String TAG = "KafkaRetryService";

    private final RealHealthDataKafkaProducer kafkaProducer;
    private final KafkaHealthCheckService healthCheckService;
    private final OfflineDataManager offlineDataManager;
    private final ScheduledExecutorService retryScheduler = Executors.newScheduledThreadPool(3);

    // Retry configuration
    private static final int MAX_RETRY_ATTEMPTS = 5;
    private static final long INITIAL_RETRY_DELAY_MS = 1000; // 1 second
    private static final double BACKOFF_MULTIPLIER = 2.0;
    private static final long MAX_RETRY_DELAY_MS = 300000; // 5 minutes
    private static final long RETRY_BATCH_INTERVAL_MS = 60000; // 1 minute

    // Statistics
    private int totalRetryAttempts = 0;
    private int successfulRetries = 0;
    private int failedRetries = 0;
    private int deadLetterCount = 0;

    @Inject
    public KafkaRetryService(
            RealHealthDataKafkaProducer kafkaProducer,
            KafkaHealthCheckService healthCheckService,
            OfflineDataManager offlineDataManager) {
        this.kafkaProducer = kafkaProducer;
        this.healthCheckService = healthCheckService;
        this.offlineDataManager = offlineDataManager;

        Log.d(TAG, "‚úÖ KafkaRetryService initialized");
        startPeriodicRetryProcessor();
    }

    // Porne»ôte procesarea periodicƒÉ a retry-urilor
    private void startPeriodicRetryProcessor() {
        Log.d(TAG, "üîÑ Starting periodic retry processor (interval: " + RETRY_BATCH_INTERVAL_MS + "ms)");

        retryScheduler.scheduleWithFixedDelay(
                this::processOfflineDataBatch,
                RETRY_BATCH_INTERVAL_MS,
                RETRY_BATCH_INTERVAL_MS,
                TimeUnit.MILLISECONDS
        );
    }

    // √éncearcƒÉ transmiterea unui singur mesaj cu retry logic
    public CompletableFuture<Boolean> retryTransmission(SensorDataDTO sensorData) {
        return retryTransmissionWithDelay(sensorData, 0);
    }

    // Retry cu delay calculat exponen»õial
    private CompletableFuture<Boolean> retryTransmissionWithDelay(SensorDataDTO sensorData, int attemptNumber) {
        if (attemptNumber >= MAX_RETRY_ATTEMPTS) {
            Log.w(TAG, "üíÄ Max retry attempts reached for sensor " + sensorData.getSensorType() +
                    " - moving to dead letter queue");
            moveToDeadLetterQueue(sensorData);
            return CompletableFuture.completedFuture(false);
        }

        // VerificƒÉ dacƒÉ Kafka este sƒÉnƒÉtos √Ænainte de retry
        if (!healthCheckService.isKafkaHealthy()) {
            Log.w(TAG, "‚ö†Ô∏è Kafka unhealthy - storing data offline for later retry");
            return storeForLaterRetry(sensorData);
        }

        // CalculeazƒÉ delay-ul cu exponential backoff
        long delayMs = calculateRetryDelay(attemptNumber);

        CompletableFuture<Boolean> future = new CompletableFuture<>();

        retryScheduler.schedule(() -> {
            totalRetryAttempts++;

            Log.d(TAG, "üîÑ Retry attempt " + (attemptNumber + 1) + "/" + MAX_RETRY_ATTEMPTS +
                    " for " + sensorData.getSensorType() + " (delay: " + delayMs + "ms)");

            kafkaProducer.sendHealthData(convertToKafkaMessage(sensorData), sensorData.getUserId())
                    .thenAccept(success -> {
                        if (success) {
                            successfulRetries++;
                            sensorData.markAsTransmitted("Kafka-Retry");

                            Log.d(TAG, "‚úÖ Retry successful for " + sensorData.getSensorType() +
                                    " after " + (attemptNumber + 1) + " attempts");
                            future.complete(true);
                        } else {
                            failedRetries++;
                            sensorData.markAsFailedTransmission("Retry failed attempt " + (attemptNumber + 1));

                            // √éncearcƒÉ din nou cu delay mai mare
                            retryTransmissionWithDelay(sensorData, attemptNumber + 1)
                                    .thenAccept(future::complete);
                        }
                    })
                    .exceptionally(throwable -> {
                        failedRetries++;
                        Log.e(TAG, "‚ùå Exception during retry attempt " + (attemptNumber + 1), throwable);

                        sensorData.markAsFailedTransmission("Exception: " + throwable.getMessage());

                        // √éncearcƒÉ din nou cu delay mai mare
                        retryTransmissionWithDelay(sensorData, attemptNumber + 1)
                                .thenAccept(future::complete);
                        return null;
                    });

        }, delayMs, TimeUnit.MILLISECONDS);

        return future;
    }

    // CalculeazƒÉ delay-ul pentru retry cu exponential backoff
    private long calculateRetryDelay(int attemptNumber) {
        long delay = (long) (INITIAL_RETRY_DELAY_MS * Math.pow(BACKOFF_MULTIPLIER, attemptNumber));
        return Math.min(delay, MAX_RETRY_DELAY_MS);
    }

    //StocheazƒÉ datele pentru retry mai t√¢rziu
    private CompletableFuture<Boolean> storeForLaterRetry(SensorDataDTO sensorData) {
        return offlineDataManager.storeOfflineData(sensorData)
                .thenApply(stored -> {
                    if (stored) {
                        Log.d(TAG, "üíæ Stored data offline for later retry: " + sensorData.getSensorType());
                    } else {
                        Log.e(TAG, "‚ùå Failed to store data offline: " + sensorData.getSensorType());
                    }
                    return stored;
                });
    }

    //ProceseazƒÉ batch-ul de date offline pentru retry
    private void processOfflineDataBatch() {
        if (!healthCheckService.isKafkaHealthy()) {
            Log.d(TAG, "‚ö†Ô∏è Kafka unhealthy - skipping offline data processing");
            return;
        }

        CompletableFuture.runAsync(() -> {
            try {
                List<OfflineHealthData> offlineData = offlineDataManager.getOfflineData().join();

                if (offlineData.isEmpty()) {
                    return;
                }

                Log.d(TAG, "üì§ Processing " + offlineData.size() + " offline records for retry");

                List<Long> successfulIds = new ArrayList<>();
                List<Long> failedIds = new ArrayList<>();

                for (OfflineHealthData data : offlineData) {
                    try {
                        // Converte»ôte din OfflineHealthData √Æn SensorDataDTO
                        SensorDataDTO sensorDataDTO = convertOfflineToDTO(data);

                        // √éncearcƒÉ transmiterea
                        boolean sent = kafkaProducer.sendHealthData(
                                convertToKafkaMessage(sensorDataDTO),
                                data.userId
                        ).join();

                        if (sent) {
                            successfulIds.add(data.id);
                            Log.d(TAG, "‚úÖ Successfully sent offline data: " + data.sensorType);
                        } else {
                            failedIds.add(data.id);
                            Log.w(TAG, "‚ùå Failed to send offline data: " + data.sensorType);
                        }

                    } catch (Exception e) {
                        failedIds.add(data.id);
                        Log.e(TAG, "‚ùå Exception sending offline data for " + data.sensorType, e);
                    }
                }

                // »òterge datele transmise cu succes
                if (!successfulIds.isEmpty()) {
                    offlineDataManager.deleteOfflineData(successfulIds).join();
                    Log.d(TAG, "üóëÔ∏è Cleaned up " + successfulIds.size() + " successfully transmitted records");
                }

                // IncrementeazƒÉ retry count pentru cele e»ôuate
                if (!failedIds.isEmpty()) {
                    offlineDataManager.incrementRetryCount(failedIds).join();
                    Log.w(TAG, "üîÑ Incremented retry count for " + failedIds.size() + " failed records");
                }

                // CurƒÉ»õƒÉ √ÆnregistrƒÉrile cu prea multe retry-uri
                int cleanedUp = offlineDataManager.cleanupFailedRecords(MAX_RETRY_ATTEMPTS).join();
                if (cleanedUp > 0) {
                    deadLetterCount += cleanedUp;
                    Log.w(TAG, "üíÄ Moved " + cleanedUp + " records to dead letter queue (total: " + deadLetterCount + ")");
                }

            } catch (Exception e) {
                Log.e(TAG, "‚ùå Error processing offline data batch", e);
            }
        });
    }

    // Converte»ôte OfflineHealthData √Æn SensorDataDTO
    private SensorDataDTO convertOfflineToDTO(OfflineHealthData offlineData) {
        SensorDataDTO dto = new SensorDataDTO();
        dto.setUserId(offlineData.userId);
        dto.setSensorType(offlineData.sensorType);
        dto.setValue(offlineData.value);
        dto.setUnit(offlineData.unit);
        dto.setTimestamp(offlineData.timestamp);
        dto.setDeviceId(offlineData.deviceId);
        dto.setRetryCount(offlineData.retryCount);
        return dto;
    }

    //Converte»ôte SensorDataDTO √Æn mesaj Kafka
    private java.util.Map<String, Object> convertToKafkaMessage(SensorDataDTO sensorData) {
        java.util.Map<String, Object> message = new java.util.HashMap<>();
        message.put("userId", sensorData.getUserId());
        message.put("sensorType", sensorData.getSensorType().getCode());
        message.put("value", sensorData.getValue());
        message.put("unit", sensorData.getUnit());
        message.put("timestamp", sensorData.getTimestamp().toString());
        message.put("deviceId", sensorData.getDeviceId());
        message.put("source", "samsung_galaxy_watch_7");
        message.put("dataType", "RETRY_SENSOR_DATA");
        message.put("retryCount", sensorData.getRetryCount());
        message.put("criticalityLevel", sensorData.getSensorType().getCriticalityLevel().name());
        return message;
    }

    // MutƒÉ datele √Æn dead letter queue
    private void moveToDeadLetterQueue(SensorDataDTO sensorData) {
        deadLetterCount++;
        try {
            Log.e(TAG, "üíÄ DEAD LETTER QUEUE: " + sensorData.getSensorType() +
                    " for user " + sensorData.getUserId() +
                    " (total dead letters: " + deadLetterCount + ")");

            // ‚úÖ 1. SalveazƒÉ √Æn OfflineDataManager cu metadata specialƒÉ
            SensorDataDTO deadLetterData = createDeadLetterCopy(sensorData);
            offlineDataManager.storeOfflineData(deadLetterData)
                    .thenAccept(stored -> {
                        if (stored) {
                            Log.d(TAG, "üíæ Dead letter stored in offline manager: " + sensorData.getSensorType());
                        }
                    });

            // ‚úÖ 2. Trimite notificare cƒÉtre monitorizare (prin log structurat)
            logDeadLetterForMonitoring(sensorData);

            // ‚úÖ 3. √éncearcƒÉ sƒÉ trimitƒÉ spre un Dead Letter Topic √Æn Kafka (op»õional)
            attemptDeadLetterTopicTransmission(sensorData);

        } catch (Exception e) {
            Log.e(TAG, "‚ùå Error processing dead letter queue for " + sensorData.getSensorType(), e);
        }
    }

    // CreeazƒÉ o copie pentru dead letter queue
    private SensorDataDTO createDeadLetterCopy(SensorDataDTO original) {
        SensorDataDTO deadLetter = new SensorDataDTO();
        deadLetter.setUserId(original.getUserId());
        deadLetter.setSensorType(original.getSensorType());
        deadLetter.setValue(original.getValue());
        deadLetter.setUnit(original.getUnit());
        deadLetter.setTimestamp(original.getTimestamp());
        deadLetter.setDeviceId(original.getDeviceId());
        deadLetter.setRetryCount(MAX_RETRY_ATTEMPTS); // Mark as max retries reached
        deadLetter.setErrorMessage("DEAD_LETTER_QUEUE: Max retries exceeded");
        deadLetter.setTransmissionMethod("DEAD_LETTER_STORAGE");

        return deadLetter;
    }

    // Log structurat pentru monitorizare
    private void logDeadLetterForMonitoring(SensorDataDTO sensorData) {
        // Log √Æn format JSON pentru parsing u»ôor de cƒÉtre sisteme de monitoring
        String monitoringLog = String.format(
                "MONITORING_ALERT: {\"type\":\"DEAD_LETTER_QUEUE\", \"sensor\":\"%s\", " +
                        "\"userId\":\"%s\", \"value\":%.2f, \"timestamp\":\"%s\", " +
                        "\"totalDeadLetters\":%d, \"severity\":\"HIGH\"}",
                sensorData.getSensorType().getCode(),
                sensorData.getUserId(),
                sensorData.getValue(),
                sensorData.getTimestamp().toString(),
                deadLetterCount
        );

        Log.w("MONITORING", monitoringLog);

        // AlertƒÉ dacƒÉ avem prea multe dead letters
        if (deadLetterCount % 10 == 0) { // Every 10 dead letters
            Log.e("MONITORING", String.format(
                    "CRITICAL_ALERT: {\"type\":\"HIGH_DEAD_LETTER_COUNT\", \"count\":%d, " +
                            "\"threshold\":10, \"action\":\"INVESTIGATE_KAFKA_ISSUES\"}",
                    deadLetterCount
            ));
        }
    }

    // √éncearcƒÉ transmiterea spre Dead Letter Topic (op»õional)
    private void attemptDeadLetterTopicTransmission(SensorDataDTO sensorData) {
        CompletableFuture.runAsync(() -> {
            try {
                // CreeazƒÉ un mesaj special pentru Dead Letter Topic
                java.util.Map<String, Object> deadLetterMessage = new java.util.HashMap<>();
                deadLetterMessage.put("messageType", "DEAD_LETTER");
                deadLetterMessage.put("originalData", convertToKafkaMessage(sensorData));
                deadLetterMessage.put("failureReason", "MAX_RETRIES_EXCEEDED");
                deadLetterMessage.put("retryCount", sensorData.getRetryCount());
                deadLetterMessage.put("deadLetterTimestamp", java.time.LocalDateTime.now().toString());
                deadLetterMessage.put("totalDeadLetters", deadLetterCount);

                // √éncearcƒÉ sƒÉ trimitƒÉ cƒÉtre un topic special pentru dead letters
                // Acest lucru e op»õional »ôi nu trebuie sƒÉ blocheze aplica»õia
                boolean sent = kafkaProducer.sendHealthData(deadLetterMessage, sensorData.getUserId()).join();

                if (sent) {
                    Log.d(TAG, "üì§ Dead letter sent to monitoring topic: " + sensorData.getSensorType());
                } else {
                    Log.w(TAG, "‚ö†Ô∏è Could not send dead letter to monitoring topic (not critical)");
                }

            } catch (Exception e) {
                // Nu logƒÉm ca eroare pentru cƒÉ e doar o func»õie de monitoring
                Log.d(TAG, "üìù Dead letter monitoring transmission failed (not critical): " + e.getMessage());
            }
        });
    }

    //Ob»õine statistici dead letter queue
    public DeadLetterStatistics getDeadLetterStatistics() {
        DeadLetterStatistics stats = new DeadLetterStatistics();
        stats.totalDeadLetters = deadLetterCount;
        stats.timestamp = java.time.LocalDateTime.now();

        if (totalRetryAttempts > 0) {
            stats.deadLetterRate = (double) deadLetterCount / totalRetryAttempts * 100.0;
        }

        return stats;
    }

    // CurƒÉ»õƒÉ dead letter queue (pentru maintenance)
    public CompletableFuture<Integer> cleanupOldDeadLetters() {
        return CompletableFuture.supplyAsync(() -> {
            try {
                // CurƒÉ»õƒÉ √ÆnregistrƒÉrile dead letter mai vechi de 24 ore
                int cleaned = offlineDataManager.cleanupFailedRecords(MAX_RETRY_ATTEMPTS + 1).join();

                if (cleaned > 0) {
                    Log.i(TAG, "üßπ Cleaned up " + cleaned + " old dead letter records");
                }

                return cleaned;

            } catch (Exception e) {
                Log.e(TAG, "‚ùå Error cleaning up dead letters", e);
                return 0;
            }
        });

    }

    // Ob»õine statisticile retry
    public RetryStatistics getRetryStatistics() {
        RetryStatistics stats = new RetryStatistics();
        stats.totalRetryAttempts = totalRetryAttempts;
        stats.successfulRetries = successfulRetries;
        stats.failedRetries = failedRetries;
        stats.deadLetterCount = deadLetterCount;

        if (totalRetryAttempts > 0) {
            stats.retrySuccessRate = (double) successfulRetries / totalRetryAttempts * 100.0;
        }

        return stats;
    }

    // ReseteazƒÉ statisticile retry
    public void resetStatistics() {
        totalRetryAttempts = 0;
        successfulRetries = 0;
        failedRetries = 0;
        deadLetterCount = 0;

        Log.d(TAG, "üîÑ Retry statistics reset");
    }

    // Opre»ôte serviciul de retry
    public void shutdown() {
        Log.d(TAG, "üõë Shutting down Kafka retry service...");

        retryScheduler.shutdown();
        try {
            if (!retryScheduler.awaitTermination(10, TimeUnit.SECONDS)) {
                retryScheduler.shutdownNow();
            }
            Log.d(TAG, "‚úÖ Kafka retry service shut down");
        } catch (InterruptedException e) {
            retryScheduler.shutdownNow();
            Log.w(TAG, "‚ö†Ô∏è Retry service shutdown interrupted", e);
        }
    }

    // Statistici retry
    public static class RetryStatistics {
        public int totalRetryAttempts = 0;
        public int successfulRetries = 0;
        public int failedRetries = 0;
        public int deadLetterCount = 0;
        public double retrySuccessRate = 0.0;

        @Override
        public String toString() {
            return String.format(
                    "Retry Stats: Total=%d, Success=%d (%.1f%%), Failed=%d, DeadLetters=%d",
                    totalRetryAttempts, successfulRetries, retrySuccessRate,
                    failedRetries, deadLetterCount
            );
        }
    }

    // ‚úÖ Clasa pentru statistici dead letter
    public static class DeadLetterStatistics {
        public int totalDeadLetters = 0;
        public double deadLetterRate = 0.0;
        public java.time.LocalDateTime timestamp;

        @Override
        public String toString() {
            return String.format("Dead Letter Stats: Total=%d, Rate=%.2f%%",
                    totalDeadLetters, deadLetterRate);
        }
    }
}